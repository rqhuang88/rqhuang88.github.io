<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Ruqi Huang</title>
</head>
<body>
<!--<div class="tab">-->
<!--    <button class="tablinks" onclick="changeCategory(event, 'Home')">Home</button>-->
<!--    <button class="tablinks" onclick="changeCategory(event, 'Publications')">Publications</button>-->
<!--    <button class="tablinks" onclick="changeCategory(event, 'Group')">Group</button>-->
<!--</div>-->

<div id="Home" class="tabcontent">
    <div class="infodiv">
        <div class="infoimg">
            <img src="img/info.png">
            <h2>Ruqi Huang</h2>
            <p><strong>Contact:</strong></p>
            <p>ruqihuang AT sz.tsinghua.edu.cn</p>
        </div>
        <div class="infotxt">
            <h3>Introduction</h3>
            <p>I obtained my PhD degree under the supervision of Frederic Chazal from University of Paris-Saclay in 2016. Prior to that, I obtained my bachelor and master degree from Tsinghua University in 2011 and 2013, respectively. My research interests lie in 3D Computer Vision and Geometry Processing, with a strong focus on developing 3D reconstruction techniques for both static and dynamic scenes. In particular, I am interested in developing learning approaches towards 3D computer vision tasks without heavy dependency on supervision, via incorporating structural priors (especially the geometric ones) into neural networks. Beyond that, I am also interested in applying geometric/topological analysis on interdisciplinary data, e.g., biological, medical and high-dimensional imaging data. My full CV can be found here.</p>
            <div class="pubclick">
                <a href="html/CV.html" class="pubbutton">[Full CV 127K]</a>
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=cgRY63gAAAAJ&view_op=list_works&sortby=pubdate" class="pubbutton">[Google Scholar]</a>
            </div>
        </div>
    </div>
</div>

<div id="News" class="tabcontent">
    <h2>News</h2>
    <div class="newsdiv">
        <div class="newstxt">
            <ul>
                <li><strong>2025-05:</strong> Two papers were accepted by npj Climate and Atmospheric Science and npj Artificial Intelligence.</li>
                <li><strong>2025-02:</strong> Two papers were accepted by CVPR 2025.</li>
                <li><strong>2025-01:</strong> One paper was accepted by Nanophotonics.</li>
                <li><strong>2024-12:</strong> One paper was accepted by IJCV 2024.</li>
            </ul>
        </div>
    </div>
</div>

<div id="Publications" class="tabcontent">
    <h2>Selected Publications</h2>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/Adaptive.png">
        </div>
        <div id="Adaptive" class="pubtxt">
            <h3 class="pubtitle">Adaptive high-resolution mapping of air pollution with a novel implicit 3D representation approach</h3>
            <p class="pubothers">Ting Zhang, Bo Zheng， Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Mapping air pollution at high spatial resolution is essential for understanding, managing, and mitigating the adverse impacts of air pollution. Current air pollution monitoring approaches suffer from limited spatial coverage and resolution. Artificial intelligence holds great promise for tackling these challenges, yet its application in air pollution monitoring remains nascent, facing limited transferability regarding low-quality labeled and non-uniform spread data. Here, we introduce Height-Field Signed Distance Function (HF-SDF), an innovative 3D implicit representation, to reconstruct air pollution concentration maps from coarse, incomplete data, which achieves both extensive spatial coverage and fine-scale results with powerful transferability. HF-SDF learns a continuous and transferable mapping model that integrates an auto-decoder network with a geometric constraint, offering flexible resolution. The evaluation uses reanalysis data and satellite observations, reaching accuracy rates of 96% and 91%, respectively. HF-SDF reveals immense promise in advancing air pollution monitoring by offering insights into the spatial heterogeneity of pollution distributions.</p>
            <p class="pubothers">npj Climate and Atmospheric Science, 2025</p>
            <div class="pubclick">
                <a href="https://www.nature.com/articles/s41612-025-01044-6?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=oa_20250513&utm_content=10.1038/s41612-025-01044-6" class="pubbutton">[pdf]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('Adaptive')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/DRiVE.png">
        </div>
        <div id="DRiVE" class="pubtxt">
            <h3 class="pubtitle">DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters</h3>
            <p class="pubothers">Mingze Sun, Junhao Chen, Junting Dong, Yurun Chen, Xinyu Jiang, Shiwei Mao, Puhua Jiang, Jingbo Wang, Bo Dai, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Recent advances in generative models have enabled high-quality 3D character reconstruction from multi-modal. However, animating these generated characters remains a challenging task, especially for complex elements like garments and hair, due to the lack of large-scale datasets and effective rigging methods. To address this gap, we curate AnimeRig, a large-scale dataset with detailed skeleton and skinning annotations. Building upon this, we propose DRiVE, a novel framework for generating and rigging 3D human characters with intricate structures. Unlike existing methods, DRiVE utilizes a 3D Gaussian representation, facilitating efficient animation and high-quality rendering. We further introduce GSDiff, a 3D Gaussian-based diffusion module that predicts joint positions as spatial distributions, overcoming the limitations of regression-based approaches. Extensive experiments demonstrate that DRiVE achieves precise rigging results, enabling realistic dynamics for clothing and hair, and surpassing previous methods in both quality and versatility. The code and dataset will be made public for academic use upon acceptance.</p>
            <p class="pubothers">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</p>
            <div class="pubclick">
                <a href="html/DRiVE.html" class="pubbutton">[pdf 1.5M]</a>
                <a href="https://driveavatar.github.io/" class="pubbutton">[webset]</a>
<!--                <a href="javascript:void(0)" onclick="redirectToPageB('DRiVE')">[BibTex]</a>-->
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/DV-Matcher.png">
        </div>
        <div id="DV-Matcher" class="pubtxt">
            <h3 class="pubtitle">DV-Matcher: Deformation-based Non-Rigid Point Cloud Matching Guided by Pre-trained Visual Features</h3>
            <p class="pubothers">Zhangquan Chen, Puhua Jiang, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> In this paper, we present DV-Matcher, a novel learning-based framework for estimating dense correspondences between non-rigidly deformable point clouds. Learning directly from unstructured point clouds without meshing or manual labelling, our framework delivers high-quality dense correspondences, which is of significant practical utility in point cloud processing. Our key contributions are two-fold: First, we propose a scheme to inject prior knowledge from pre-trained vision models into geometric feature learning, which effectively complements the local nature of geometric features with global and semantic information; Second, we propose a novel deformation-based module to promote the extrinsic alignment induced by the learned correspondences, which effectively enhances the feature learning. Experimental results show that our method achieves state-of-the-art results in matching non-rigid point clouds in both near-isometric and heterogeneous shape collection as well as more realistic partial and noisy data.</p>
            <p class="pubothers">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025</p>
            <div class="pubclick">
                <a href="html/DV-Matcher.html" class="pubbutton">[pdf 3.4M]</a>
                <a href="https://github.com/rqhuang88/DV-Matcher" class="pubbutton">[code]</a>
                <a href="https://drive.google.com/drive/folders/1CK9qihI2yyxkuXsxSHqzTRhLdTk8qghn" class="pubbutton">[dataset]</a>
<!--                <a href="javascript:void(0)" onclick="redirectToPageB('DV-Matcher')">[BibTex]</a>-->
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/EMNN.jpg">
        </div>
        <div id="EMNN" class="pubtxt">
            <h3 class="pubtitle">Reliable, efficient, and scalable photonic inverse design empowered by physics-inspired deep learning</h3>
            <p class="pubothers">Guocheng Shao, Tiankuang Zhou, Tao Yan, Yanchen Guo, Yun Zhao, Ruqi Huang and Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> On-chip computing metasystems composed of
                multilayer metamaterials have the potential to become the
                next-generation computing hardware endowed with lightspeed processing ability and low power consumption but
                are hindered by current design paradigms. To date, neither
                numerical nor analytical methods can balance efficiency
                and accuracy of the design process. To address the issue, a
                physics-inspired deep learning architecture termed electromagnetic neural network (EMNN) is proposed to enable an
                efficient, reliable, and flexible paradigm of inverse design.
                EMNN consists of two parts: EMNN Netlet serves as a local
                electromagnetic field solver; Huygens–Fresnel Stitch is used
                for concatenating local predictions. It can make direct,
                rapid, and accurate predictions of full-wave field based on
                input fields of arbitrary variations and structures of non-fixed size. With the aid of EMNN, we design computing metasystems that can perform handwritten digit recognition and speech command recognition. EMNN increases the
                design speed by 17,000 times than that of the analytical
                model and reduces the modeling error by two orders of
                magnitude compared to the numerical model. By integrating deep learning techniques with fundamental physical
                principle, EMNN manifests great interpretability and generalization ability beyond conventional networks. Additionally, it innovates a design paradigm that guarantees both
                high efficiency and high fidelity. Furthermore, the flexible
                paradigm can be applicable to the unprecedentedly challenging design of large-scale, high-degree-of-freedom, and
                functionally complex devices embodied by on-chip optical
                diffractive networks, so as to further promote the development of computing metasystems.</p>
            <p class="pubothers">Nanophotonics</p>
            <div class="pubclick">
                <a href="https://www.degruyter.com/document/doi/10.1515/nanoph-2024-0504/html" class="pubbutton">[pdf 4.2M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('EMNN')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/BeyondTalking.png">
        </div>
        <div id="BeyondTalking" class="pubtxt">
            <h3 class="pubtitle">Beyond Talking – Generating Holistic 3D Human Dyadic Motion for Communication</h3>
            <p class="pubothers">Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions
                for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the
                combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements.
                We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time
                mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model
                specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both
                the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse.
                Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo
                holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released
                for research purposes upon acceptance. </p>
            <p class="pubothers">International Journal of Computer Vision, 2024</p>
            <div class="pubclick">
                <a href="html/BeyondTalking.html" class="pubbutton">[pdf 2.1M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('BeyondTalking')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/SRIF.png">
        </div>
        <div id="SRIF" class="pubtxt">
            <h3 class="pubtitle">SRIF: Semantic Shape Registration Empowered by Diffusion-based Image Morphing and Flow Estimation</h3>
            <p class="pubothers">Mingze Sun, Chen Guo, Puhua Jiang, Shiwei Mao, Yurun Chen, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> In this paper, we propose SRIF, a novel Semantic shape Registration framework based on diffusion-based Image morphing and Flow estimation. More
                concretely, given a pair of extrinsically aligned shapes, we first render them
                from multi-views, and then utilize an image interpolation framework based
                on diffusion models to generate sequences of intermediate images between
                them. The images are later fed into a dynamic 3D Gaussian splatting framework, with which we reconstruct and post-process for intermediate point
                clouds respecting the image morphing processing. In the end, tailored for
                the above, we propose a novel registration module to estimate continuous normalizing flow, which deforms source shape consistently towards
                the target, with intermediate point clouds as weak guidance. Our key insight is to leverage large vision models (LVMs) to associate shapes and
                therefore obtain much richer semantic information on the relationship between shapes than the ad-hoc feature extraction and alignment. As consequence, SRIF achieves high-quality dense correspondences on challenging
                shape pairs, but also delivers smooth, semantically meaningful interpolation in between. Empirical evidences justify the effectiveness and superiority of our method as well as specific design choices. </p>
            <p class="pubothers">Proc. SIGGRAPH Asia, 2024</p>
            <div class="pubclick">
                <a href="html/SRIF.html" class="pubbutton">[pdf 3.2M]</a>
                <a href="https://github.com/rqhuang88/SRIF" class="pubbutton">[code]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('SRIF')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/EHOF.png">
        </div>
        <div id="EHOF" class="pubtxt">
            <h3 class="pubtitle">Enhancing Human Optical Flow via 3D Spectral Prior</h3>
            <p class="pubothers">Shiwei Mao, Mingze Sun, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> In this paper, we consider the problem of human optical flow estimation, which is critical in a series of human-centric computer vision tasks. Recent deep learning-based optical flow models have achieved considerable accuracy and generalization by
                incorporating various kinds of priors. However, the majority either rely on large-scale 2D annotations or rigid priors, overlooking the 3D non-rigid nature of human articulations. To this end, we advocate enhancing human optical flow estimation via
                3D spectral prior-aware pretraining, which is based on the well-known functional maps formulation in 3D shape matching. Our
                pretraining can be performed with synthetic human shapes. More specifically, we first render shapes to images and then leverage
                the natural inclusion maps from images to shapes to lift 2D optical flow into 3D correspondences, which are further encoded
                as functional maps. Such lifting operation allows to inject the intrinsic geometric features encoded in the spectral representations into optical flow learning, leading to improvement of the latter, especially in the presence of non-rigid deformations.
                In practice, we establish a pretraining pipeline tailored for triangular meshes, which is general regarding target optical flow
                network. It is worth noting that it does not introduce any additional learning parameters but only require some pre-computed
                eigen decomposition on the meshes. For RAFT and GMA, our pretraining task achieves improvements of 12.8% and 4.9% in
                AEPE on the SHOF benchmark, respectively.</p>
            <p class="pubothers">Proc. Pacific Graphics, 2024</p>
            <div class="pubclick">
                <a href="html/EHOF.html" class="pubbutton">[pdf 7.7M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('EHOF')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/Nanowatt.png">
        </div>
        <div id="Nanowatt" class="pubtxt">
            <h3 class="pubtitle">Nanowatt all-optical 3D perception for mobile robotics</h3>
            <p class="pubothers">Tao Yan, Tiankuang Zhou, Yanchen Guo, Yun Zhao, Guocheng Shao, Jiamin Wu, Ruqi Huang, Qionghai Dai, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Three-dimensional (3D) perception is vital to drive mobile robotics’ progress toward intelligence. However, state-of-the-art 3D perception solutions require complicated postprocessing or point-by-point scanning, suffering computational burden, latency of tens of milliseconds, and additional power consumption. Here, we propose a parallel all-optical computational chipset 3D perception architecture (Aop3D) with nanowatt power and light speed. The 3D perception is executed during the light propagation over the passive chipset, and the captured light intensity distribution provides a direct reflection of the depth map, eliminating the need for extensive postprocessing. The prototype system of Aop3D is tested in various scenarios and deployed to a mobile robot, demonstrating unprecedented performance in distance detection and obstacle avoidance. Moreover, Aop3D works at a frame rate of 600 hertz and a power consumption of 33.3 nanowatts per meta-pixel experimentally. Our work is promising toward next-generation direct 3D perception techniques with light speed and high energy efficiency.</p>
            <p class="pubothers">Science Advances, 2024</p>
            <div class="pubclick">
                <a href="https://www.science.org/doi/abs/10.1126/sciadv.adn2031" class="pubbutton">[pdf]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('Nanowatt')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/XScale-NVS.png">
        </div>
        <div id="XScale-NVS" class="pubtxt">
            <h3 class="pubtitle">XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold</h3>
            <p class="pubothers">Guangyu Wang, Jinzhi Zhang, Fan Wang, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> We propose XScale-NVS for high-fidelity cross-scale
                novel view synthesis of real-world large-scale scenes. Existing
                representations based on explicit surface suffer from
                discretization resolution or UV distortion, while implicit
                volumetric representations lack scalability for large scenes
                due to the dispersed weight distribution and surface ambiguity.
                In light of the above challenges, we introduce hash
                featurized manifold, a novel hash-based featurization coupled
                with a deferred neural rendering framework. This approach
                fully unlocks the expressivity of the representation
                by explicitly concentrating the hash entries on the 2D manifold,
                thus effectively representing highly detailed contents
                independent of the discretization resolution. We also introduce
                a novel dataset, namely GigaNVS, to benchmark
                cross-scale, high-resolution novel view synthesis of realworld
                large-scale scenes. Our method significantly outperforms
                competing baselines on various real-world scenes,
                yielding an average LPIPS that is ∼ 40% lower than prior
                state-of-the-art on the challenging GigaNVS benchmark.</p>
            <p class="pubothers">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</p>
            <div class="pubclick">
                <a href="html/XScale-NVS.html" class="pubbutton">[pdf 8.9M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('XScale-NVS')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/OmniSeg3D.png">
        </div>
        <div id="OmniSeg3D" class="pubtxt">
            <h3 class="pubtitle">OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</h3>
            <p class="pubothers">Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Towards holistic understanding of 3D scenes, a general
                3D segmentation method is needed that can segment diverse
                objects without restrictions on object quantity or categories,
                while also reflecting the inherent hierarchical structure. To
                achieve this, we propose OmniSeg3D, an omniversal segmentation
                method aims for segmenting anything in 3D all
                at once. The key insight is to lift multi-view inconsistent 2D
                segmentations into a consistent 3D feature field through a
                hierarchical contrastive learning framework, which is accomplished
                by two steps. Firstly, we design a novel hierarchical
                representation based on category-agnostic 2D segmentations to model the multi-level relationship among pixels.
                Secondly, image features rendered from the 3D feature
                field are clustered at different levels, which can be further
                drawn closer or pushed apart according to the hierarchical
                relationship between different levels. In tackling
                the challenges posed by inconsistent 2D segmentations, this
                framework yields a global consistent 3D feature field, which
                further enables hierarchical segmentation, multi-object selection,
                and global discretization. Extensive experiments
                demonstrate the effectiveness of our method on high-quality
                3D segmentation and accurate hierarchical structure understanding.
                A graphical user interface further facilitates
                flexible interaction for omniversal 3D segmentation.</p>
            <p class="pubothers">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</p>
            <div class="pubclick">
                <a href="html/OmniSeg3D.html" class="pubbutton">[pdf 9.0M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('OmniSeg3D')">[BibTex]</a>
            </div>
        </div>
    </div>
<!--    <div class="pubdiv">-->
<!--        <div class="pubimg">-->
<!--            <img width="200px" src="img/ULPCMM.png">-->
<!--        </div>-->
<!--        <div id="ULPCMM" class="pubtxt">-->
<!--            <h3 class="pubtitle">Unsupervised learning of pixel clustering in Mueller matrix images for mapping microstructural features in pathological tissues</h3>-->
<!--            <p class="pubothers">Jiachen Wan, Yang Dong, Yue Yao, Weijin Xiao, Ruqi Huang, Jing-Hao Xue, Ran Peng, Haojie Pei, Xuewu Tian, Ran Liao, Honghui He, Nan Zeng, Chao Li, Hui Ma</p>-->
<!--            <p class="pubabstract"><strong>Abstract:</strong> In histopathology, doctors identify diseases by characterizing abnormal cells and their spatial organization within tissues. Polarization microscopy and supervised learning have been proved as an effective tool for extracting polarization parameters to highlight pathological features. Here, we present an alternative approach based on unsupervised learning to group polarization-pixels into clusters, which correspond to distinct pathological structures. For pathological samples from different patients, it is confirmed that such unsupervised learning technique can decompose the histological structures into a stable basis of characteristic microstructural clusters, some of which correspond to distinctive pathological features for clinical diagnosis. Using hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) samples, we demonstrate how the proposed framework can be utilized for segmentation of histological image, visualization of microstructure composition associated with lesion, and identification of polarization-based microstructure markers that correlates with specific pathology variation. This technique is capable of unraveling invisible microstructures in non-polarization images, and turn them into visible polarization features to pathologists and researchers.</p>-->
<!--            <p class="pubothers">Proc. Communications Engineering, 2023</p>-->
<!--            <div class="pubclick">-->
<!--                <a href="html/ULPCMM.html" class="pubbutton">[pdf 3.2M]</a>-->
<!--                <a href="javascript:void(0)" onclick="redirectToPageB('ULPCMM')">[BibTex]</a>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/DFR.png">
        </div>
        <div id="DFR" class="pubtxt">
            <h3 class="pubtitle">Non-Rigid Shape Registration via Deep Functional Maps Prior</h3>
            <p class="pubothers">Puhua Jiang, Mingze Sun, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> In this paper, we propose a learning-based framework for non-rigid shape registration
                without correspondence supervision. Traditional shape registration techniques
                typically rely on correspondences induced by extrinsic proximity, therefore can
                fail in the presence of large intrinsic deformations. Spectral mapping methods
                overcome this challenge by embedding shapes into, geometric or learned, highdimensional
                spaces, where shapes are easier to align. However, due to the dependency
                on abstract, non-linear embedding schemes, the latter can be vulnerable with
                respect to perturbed or alien input. In light of this, our framework takes the best
                of both worlds. Namely, we deform source mesh towards the target point cloud,
                guided by correspondences induced by high-dimensional embeddings learned from
                deep functional maps (DFM). In particular, the correspondences are dynamically
                updated according to the intermediate registrations and filtered by consistency prior,
                which prominently robustify the overall pipeline. Moreover, in order to alleviate the
                requirement of extrinsically aligned input, we train an orientation regressor on a set
                of aligned synthetic shapes independent of the training shapes for DFM. Empirical
                results show that, with as few as dozens of training shapes of limited variability, our
                pipeline achieves state-of-the-art results on several benchmarks of non-rigid point
                cloud matching, but also delivers high-quality correspondences between unseen
                challenging shape pairs that undergo both significant extrinsic and intrinsic deformations,
                in which case neither traditional registration methods nor intrinsic methods
                work.</p>
            <p class="pubothers">Proc. 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</p>
            <div class="pubclick">
                <a href="html/DFR.html" class="pubbutton">[pdf 9.9M]</a>
                <a href="https://github.com/rqhuang88/DFR" class="pubbutton">[code]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('DFR')">[BibTex]</a>
            </div>
<!--            &lt;!&ndash;悬浮窗口&ndash;&gt;-->
<!--            <div id="window_DFR" class="window_css">-->
<!--                <div style="text-align:right;">-->
<!--                    <a onclick="hideWindow('window_DFR')">-->
<!--                        <img width="3%" src="img/close.png">-->
<!--                    </a>-->
<!--                    <p class="bib_css">-->
<!--                        @article{jiang2023non,-->
<!--                        title={Non-Rigid Shape Registration via Deep Functional Maps Prior},-->
<!--                        author={Jiang, Puhua and Sun, Mingze and Huang, Ruqi},-->
<!--                        journal={arXiv preprint arXiv:2311.04494},-->
<!--                        year={2023}-->
<!--                        }-->
<!--                    </p>-->
<!--                    <button onclick="copyToClipboard('window_DFR')">Copy</button>-->
<!--                </div>-->
<!--            </div>-->
<!--            &lt;!&ndash;出现悬浮窗口后,背景变暗&ndash;&gt;-->
<!--            <div id="shadow" class="shadow_css"></div>-->
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/GiganticNVS.png">
        </div>
        <div id="GiganticNVS" class="pubtxt">
            <h3 class="pubtitle">GiganticNVS: Gigapixel Large-scale Neural Rendering with Implicit Meta-deformed Manifold</h3>
            <p class="pubothers">Guangyu Wang, Jinzhi Zhang, Kai Zhang, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> The rapid advances of high-performance sensation empowered gigapixel-level imaging/videography for large-scale scenes,
                yet the abundant details in gigapixel images were rarely valued in 3d reconstruction solutions. Bridging the gap between the sensation
                capacity and that of reconstruction requires to attack the large-baseline challenge imposed by the large-scale scenes, while utilizing
                the high-resolution details provided by the gigapixel images. This paper introduces GiganticNVS for gigapixel large-scale novel view
                synthesis (NVS). Existing NVS methods suffer from excessively blurred artifacts and fail on the full exploitation of image resolution, due
                to their inefficacy of recovering a faithful underlying geometry and the dependence on dense observations to accurately interpolate
                radiance. Our key insight is that, a highly-expressive implicit field with view-consistency is critical for synthesizing high-fidelity details
                from large-baseline observations. In light of this, we propose meta-deformed manifold, where meta refers to the locally defined surface
                manifold whose geometry and appearance are embedded into high-dimensional latent space. Technically, meta can be decoded as
                neural fields using an MLP (i.e., implicit representation). Upon this novel representation, multi-view geometric correspondence can be
                effectively enforced with featuremetric deformation and the reflectance field can be learned purely on the surface. Experimental results
                verify that the proposed method outperforms state-of-the-art methods both quantitatively and qualitatively, not only on the standard
                datasets containing complex real-world scenes with large baseline angles, but also on the challenging gigapixel-level ultra-large-scale
                benchmarks.</p>
            <p class="pubothers">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</p>
            <div class="pubclick">
                <a href="html/GiganticNVS.html" class="pubbutton">[pdf 2.4M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('GiganticNVS')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/SSCDFMAP.png">
        </div>
        <div id="SSCDFMAP" class="pubtxt">
            <h3 class="pubtitle">Spatially and Spectrally Consistent Deep Functional Maps</h3>
            <p class="pubothers">Mingze Sun, Shiwei Mao, Puhua Jiang, Maks Ovsjanikov, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Cycle consistency has long been exploited as a powerful
                prior for jointly optimizing maps within a collection
                of shapes. In this paper, we investigate its utility in the
                approaches of Deep Functional Maps, which are considered
                state-of-the-art in non-rigid shape matching. We first
                justify that under certain conditions, the learned maps,
                when represented in the spectral domain, are already cycle
                consistent. Furthermore, we identify the discrepancy that
                spectrally consistent maps are not necessarily spatially, or
                point-wise, consistent. In light of this, we present a novel
                design of unsupervised Deep Functional Maps, which effectively
                enforces the harmony of learned maps under the
                spectral and the point-wise representation. By taking advantage
                of cycle consistency, our framework produces stateof-
                the-art results in mapping shapes even under significant
                distortions. Beyond that, by independently estimating maps
                in both spectral and spatial domains, our method naturally
                alleviates over-fitting in network training, yielding superior
                generalization performance and accuracy within an
                array of challenging tests for both near-isometric and nonisometric
                datasets.</p>
            <p class="pubothers">Proc. International Conference on Computer Vision (ICCV), 2023</p>
            <div class="pubclick">
                <a href="html/SSCDFMAP.html" class="pubbutton">[pdf 16.6M]</a>
                <a href="https://github.com/rqhuang88/Spatially-and-Spectrally-Consistent-Deep-Functional-Maps" class="pubbutton">[code]</a>
                <a href="html/SSCDFMAP_poster.html" class="pubbutton">[poster 9.2M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('SSCDFMAP')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/RealGraph.png">
        </div>
        <div id="RealGraph" class="pubtxt">
            <h3 class="pubtitle">RealGraph: A Multiview Dataset for 4D Real-world Context Graph Generation</h3>
            <p class="pubothers">Haozhe Lin, Zequn Chen, Jinzhi Zhang, Bing Bai, Yu Wang, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Understanding 4D scene context in real world has become
                urgently critical for deploying sophisticated AI systems.
                In this paper, we propose a brand new scene understanding
                paradigm called “Context Graph Generation
                (CGG)”, aiming at abstracting holistic semantic information
                in the complicated 4D world. The CGG task capitalizes
                on the calibrated multiview videos of a dynamic scene, and
                targets at recovering semantic information (coordination,
                trajectories and relationships) of the presented objects in
                the form of spatio-temporal context graph in 4D space. We
                also present a benchmark 4D video dataset “RealGraph”,
                the first dataset tailored for the proposed CGG task. The
                raw data of RealGraph is composed of calibrated and synchronized
                multiview videos. We exclusively provide manual
                annotations including object 2D&3D bounding boxes, category
                labels and semantic relationships. We also make sure
                the annotated ID for every single object is temporally and
                spatially consistent. We propose the first CGG baseline algorithm,
                Multiview-based Context Graph Generation Network
                (MCGNet), to empirically investigate the legitimacy
                of CGG task on RealGraph dataset. We nevertheless reveal
                the great challenges behind this task and encourage
                the community to explore beyond our solution.</p>
            <p class="pubothers">Proc. International Conference on Computer Vision (ICCV), 2023</p>
            <div class="pubclick">
                <a href="html/RealGraph.html" class="pubbutton">[pdf 2.0M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('RealGraph')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/GIF.jpg">
        </div>
        <div id="GIF" class="pubtxt">
            <h3 class="pubtitle">The Group Interaction Field for Learning and Explaining Pedestrian Anticipation</h3>
            <p class="pubothers">Xueyang Wang, Xuecheng Chen, Puhua Jiang, Haozhe Lin, Xiaoyun Yuan, Mengqi Ji, Yuchen Guo, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong> Anticipating others’ actions is innate and essential in order for humans to navigate and interact well with others in dense crowds. This ability is urgently required for unmanned systems such as service robots and self-driving cars. However, existing solutions struggle to predict pedestrian anticipation accurately, because the influence of group-related social behaviors has not been well considered. While group relationships and group interactions are ubiquitous and significantly influence pedestrian anticipation, their influence is diverse and subtle, making it difficult to explicitly quantify. Here, we propose the group interaction field (GIF), a novel group-aware representation that quantifies pedestrian anticipation into a probability field of pedestrians’ future locations and attention orientations. An end-to-end neural network, GIFNet, is tailored to estimate the GIF from explicit multidimensional observations. GIFNet quantifies the influence of group behaviors by formulating a group interaction graph with propagation and graph attention that is adaptive to the group size and dynamic interaction states. The experimental results show that the GIF effectively represents the change in pedestrians’ anticipation under the prominent impact of group behaviors and accurately predicts pedestrians’ future states. Moreover, the GIF contributes to explaining various predictions of pedestrians’ behavior in different social states. The proposed GIF will eventually be able to allow unmanned systems to work in a human-like manner and comply with social norms, thereby promoting harmonious human–machine relationships.</p>
            <p class="pubothers">Engineering, 2023</p>
            <div class="pubclick">
                <a href="https://doi.org/10.1016/j.eng.2023.05.020" class="pubbutton">[pdf 3.6M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('GIF')">[BibTex]</a>
            </div>
        </div>
    </div>
<!--    <div class="pubdiv">-->
<!--        <div class="pubimg">-->
<!--            <img width="200px" src="img/FastP2P.png">-->
<!--        </div>-->
<!--        <div id="FastP2P" class="pubtxt">-->
<!--            <h3 class="pubtitle">Fast Point Cloud Registration for Urban Scenes via Pillar-Point Representation</h3>-->
<!--            <p class="pubothers">Siyuan Gu, Ruqi Huang</p>-->
<!--            <p class="pubabstract"><strong>Abstract:</strong> Efficient and robust point cloud registration is an essential task for-->
<!--                real-time applications in urban scenes. Most methods introduce keypoint sampling-->
<!--                or detection to achieve real-time registration of large-scale point clouds.-->
<!--                Recent advances in keypoint-free methods have succeeded in alleviating the bias-->
<!--                and error introduced by keypoint detection via coarse-to-fine dense matching-->
<!--                strategies. Nevertheless, the running time performance of such a strategy turns-->
<!--                out to be far inferior to keypoint methods. This paper proposes a novel framework-->
<!--                that adopts a pillar-point representation based feature extraction pipeline-->
<!--                and a three-stage semi-dense keypoint matching scheme. The scheme includes-->
<!--                global coarse matching, anchor generation and local dense matching for efficient-->
<!--                correspondence matching. Experiments on large-scale outdoor datasets, including-->
<!--                KITTI and NuScenes, demonstrate that the proposed feature representation-->
<!--                and matching framework achieve real-time inference and high registration recall.</p>-->
<!--            <p class="pubothers">Proc. CAAI International Conference on Artificial Intelligence(CICAI), 2023</p>-->
<!--            <div class="pubclick">-->
<!--                <a href="html/FastP2P.html" class="pubbutton">[pdf 0.48M]</a>-->
<!--&lt;!&ndash;                <a href="javascript:void(0)" onclick="redirectToPageB('FastP2P')">[BibTex]</a>&ndash;&gt;-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/NIE.png">
        </div>
        <div id="NIE" class="pubtxt">
            <h3 class="pubtitle">Neural Intrinsic Embedding for Non-rigid Point Cloud Matching</h3>
            <p class="pubothers">Puhua Jiang, Mingze Sun, Ruqi Huang</p>
            <p class="pubabstract"><strong>Abstract:</strong> As a primitive 3D data representation, point clouds
                are prevailing in 3D sensing, yet short of intrinsic structural
                information of the underlying objects. Such discrepancy
                poses great challenges on directly establishing correspondences
                between point clouds sampled from deformable
                shapes. In light of this, we propose Neural Intrinsic Embedding
                (NIE) to embed each vertex into a high-dimensional
                space in a way that respects the intrinsic structure. Based
                upon NIE, we further present a weakly-supervised learning
                framework for non-rigid point cloud registration. Unlike
                the prior works, we do not require expansive and sensitive
                off-line basis construction (e.g., eigen-decomposition
                of Laplacians), nor do we require ground-truth correspondence
                labels for supervision. We empirically show that our
                framework performs on par with or even better than the
                state-of-the-art baselines, which generally require more supervision
                and/or more structural geometric input.</p>
            <p class="pubothers">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
            <div class="pubclick">
                <a href="html/NIE.html" class="pubbutton">[pdf 6.5M]</a>
                <a href="https://github.com/TBSI2f/Neural-Intrinsic-Embedding" class="pubbutton">[code]</a>
                <a href="html/NIE_poster.html" class="pubbutton">[poster 2.4M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('NIE')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/ONODE.png">
        </div>
        <div id="ONODE" class="pubtxt">
            <h3 class="pubtitle">Optical Neural Ordinary Differential Equations</h3>
            <p class="pubothers">Yun Zhao, Hang Chen, Min Lin, Haiou Zhang, Tao Yan, Xing Lin, Ruqi Huang, Qionghai Dai</p>
            <p class="pubabstract"><strong>Abstract:</strong>  Increasing the layer number of on-chip photonic neural networks (PNNs) is essential to improve its model performance. However, the successively cascading of network hidden layers results in larger integrated photonic chip areas. To address this issue, we propose the optical neural ordinary differential equations (ON-ODE) architecture that parameterizes the continuous dynamics of hidden layers with optical ODE solvers. The ON-ODE comprises the PNNs followed by the photonic integrator and optical feedback loop, which can be configured to represent residual neural networks (ResNet) and recurrent neural networks with effectively reduced chip area occupancy. For the interference-based optoelectronic nonlinear hidden layer, the numerical experiments demonstrate that the single hidden layer ON-ODE can achieve approximately the same accuracy as the two-layer optical ResNet in image classification tasks. Besides, the ON-ODE improves the model classification accuracy for the diffraction-based all-optical linear hidden layer. The time-dependent dynamics property of ON-ODE is further applied for trajectory prediction with high accuracy.</p>
            <p class="pubothers">Optics Letters, 2023</p>
            <div class="pubclick">
                <a href="html/ONODE.html" class="pubbutton">[pdf 1.0M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('ONODE')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/ElasticMVS.png">
        </div>
        <div id="ElasticMVS" class="pubtxt">
            <h3 class="pubtitle">ElasticMVS: Learning elastic part representation for self-supervised multi-view stereopsis</h3>
            <p class="pubothers">Jinzhi Zhang, Ruofan Tang, Zheng Cao, Jing Xiao, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong>  Learning
                dense surface predictions from only a set of images without onerous groundtruth
                3D training data for supervision. However, existing methods highly rely on
                the local photometric consistency, which fail to identify accurately dense correspondence
                in broad textureless or reflectant areas. In this paper, we show that geometric
                proximity such as surface connectedness and occlusion boundaries implicitly inferred
                from images could serve as reliable guidance for pixel-wise multi-view
                correspondences. With this insight, we present a novel elastic part representation,
                which encodes physically-connected part segmentations with elastically-varying
                scales, shapes and boundaries. Meanwhile, a self-supervised MVS framework
                namely ElasticMVS is proposed to learn the representation and estimate per-view
                depth following a part-aware propagation and evaluation scheme. Specifically, the
                pixel-wise part representation is trained by a contrastive learning-based strategy,
                which increases the representation compactness in geometrically concentrated areas
                and contrasts otherwise. ElasticMVS iteratively optimizes a part-level consistency
                loss and a surface smoothness loss, based on a set of depth hypotheses propagated
                from the geometrically concentrated parts. Extensive evaluations convey the
                superiority of ElasticMVS in the reconstruction completeness and accuracy, as
                well as the efficiency and scalability. Particularly, for the challenging large-scale
                reconstruction benchmark, ElasticMVS demonstrates significant performance gain
                over both the supervised and self-supervised approaches.</p>
            <p class="pubothers">Proc. Conference on Neural Information Processing Systems (NeurIPS) (Spotlight), 2022</p>
            <div class="pubclick">
                <a href="html/ElasticMVS.html" class="pubbutton">[pdf 6.5M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('ElasticMVS')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/ParseMVS.png">
        </div>
        <div id="ParseMVS" class="pubtxt">
            <h3 class="pubtitle">ParseMVS: Learning Primitive-aware Surface Representations for Sparse Multi-view Stereopsis</h3>
            <p class="pubothers">Haiyang Ying, Jinzhi Zhang, Yuzhe Chen, Zheng Cao, Jing Xiao, Ruqi Huang, Lu Fang</p>
            <p class="pubabstract"><strong>Abstract:</strong>  Multi-view stereopsis (MVS) recovers 3D surfaces by finding dense
                photo-consistent correspondences from densely sampled images.
                In this paper, we tackle the challenging MVS task from sparsely
                sampled views (up to an order of magnitude fewer images), which
                is more practical and cost-efficient in applications. The major challenge
                comes from the significant correspondence ambiguity introduced
                by the severe occlusions and the highly skewed patches. On
                the other hand, such ambiguity can be resolved by incorporating
                geometric cues from the global structure. In light of this, we propose
                ParseMVS, boosting sparse MVS by learning the Primitive-AwaRe
                Surface rEpresentation. In particular, on top of being aware of global
                structure, our novel representation further allows for the preservation
                of fine details including geometry, texture, and visibility.
                More specifically, the whole scene is parsed into multiple geometric
                primitives. On each of them, the geometry is defined as the displacement
                along the primitives’ normal directions, together with the
                texture and visibility along each view direction. An unsupervised
                neural network is trained to learn these factors by progressively
                increasing the photo-consistency and render-consistency among
                all input images. Since the surface properties are changed locally
                in the 2D space of each primitive, ParseMVS can preserve global
                primitive structures while optimizing local details, handling the
                ‘incompleteness’ and the ‘inaccuracy’ problems.We experimentally
                demonstrate that ParseMVS constantly outperforms the state-ofthe-
                art surface reconstruction method in both completeness and the
                overall score under varying sampling sparsity, especially under the extreme sparse-MVS settings. Beyond that, ParseMVS also shows
                great potential in compression, robustness, and efficiency.</p>
            <p class="pubothers">Proc. ACM International Conference on Multimedia, 2022</p>
            <div class="pubclick">
                <a href="html/ParseMVS.html" class="pubbutton">[pdf 22.5M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('ParseMVS')">[BibTex]</a>
            </div>
        </div>
    </div>
<!--    <div class="pubdiv">-->
<!--        <div class="pubimg">-->
<!--            <img width="200px" src="img/CCDC.png">-->
<!--        </div>-->
<!--        <div id="CCDC" class="pubtxt">-->
<!--            <h3 class="pubtitle">Cross-Camera Deep Colorization</h3>-->
<!--            <p class="pubothers">Yaping Zhao, Haitian Zheng, Mengqi Ji, Ruqi Huang</p>-->
<!--            <p class="pubabstract"><strong>Abstract:</strong>  In this paper, we consider the color-plus-mono dual-camera system-->
<!--                and propose an end-to-end convolutional neural network to align and fuse images-->
<!--                from it in an efficient and cost-effective way. Our method takes cross-domain and-->
<!--                cross-scale images as input, and consequently synthesizes HR colorization results-->
<!--                to facilitate the trade-off between spatial-temporal resolution and color depth in-->
<!--                the single-camera imaging system. In contrast to the previous colorization methods,-->
<!--                ours can adapt to color and monochrome cameras with distinctive spatialtemporal-->
<!--                resolutions, rendering the flexibility and robustness in practical applications.-->
<!--                The key ingredient of our method is a cross-camera alignment module-->
<!--                that generates multi-scale correspondences for cross-domain image alignment.-->
<!--                Through extensive experiments on various datasets and multiple settings, we validate-->
<!--                the flexibility and effectiveness of our approach. Remarkably, our method-->
<!--                consistently achieves substantial improvements, i.e., around 10dB PSNR gain,-->
<!--                upon the state-of-the-art methods.</p>-->
<!--            <p class="pubothers">Proc. CAAI International Conference on Artificial Intelligence(Oral presentation), 2022</p>-->
<!--            <div class="pubclick">-->
<!--                <a href="html/CCDC.html" class="pubbutton">[pdf 9.2M]</a>-->
<!--                <a href="https://github.com/IndigoPurple/CCDC" class="pubbutton">[code]</a>-->
<!--                <a href="javascript:void(0)" onclick="redirectToPageB('CCDC')">[BibTex]</a>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
<!--    <div class="pubdiv">-->
<!--        <div class="pubimg">-->
<!--            <img width="200px" src="img/EFENet.png">-->
<!--        </div>-->
<!--        <div id="EFENet" class="pubtxt">-->
<!--            <h3 class="pubtitle">EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation</h3>-->
<!--            <p class="pubothers">Yaping Zhao, Mengqi Ji, Ruqi Huang, Bin Wang, Shengjin Wang</p>-->
<!--            <p class="pubabstract"><strong>Abstract:</strong>  In this paper, we consider the problem of reference-based video super-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference frame to super-resolve a low-resolution (LR) video sequence. The existing approaches to RefVSR essentially attempt to align the reference and the input sequence, in the presence of resolution gap and long temporal range. However, they either ignore temporal structure within the input sequence, or suffer accumulative alignment errors. To address these issues, we propose EFENet to exploit simultaneously the visual cues contained in the HR reference and the temporal information contained in the LR sequence. EFENet first globally estimates cross-scale flow between the reference and each LR frame. Then our novel flow refinement module of EFENet refines the flow regarding the furthest frame using all the estimated flows, which leverages the global temporal information within the sequence and therefore effectively reduces the alignment errors. We provide comprehensive evaluations to validate the strengths of our approach, and to demonstrate that the proposed framework outperforms the state-of-the-art methods.</p>-->
<!--            <p class="pubothers">Proc. CAAI International Conference on Artificial Intelligence, 2021</p>-->
<!--            <div class="pubclick">-->
<!--                <a href="html/EFENet.html" class="pubbutton">[pdf 3.0M]</a>-->
<!--                <a href="javascript:void(0)" onclick="redirectToPageB('EFENet')">[BibTex]</a>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/ConsistentZoomOut.png">
        </div>
        <div id="ConsistentZoomOut" class="pubtxt">
            <h3 class="pubtitle">Consistent ZoomOut: Efficient Spectral Map Synchronization</h3>
            <p class="pubothers">Ruqi Huang, Jing Ren, Peter Wonka, Maks Ovsjanikov</p>
            <p class="pubabstract"><strong>Abstract:</strong>  In this paper, we propose a novel method, which we call CONSISTENT ZOOMOUT, for efficiently refining correspondences among deformable 3D shape collections, while promoting the resulting map consistency. Our formulation is closely related to a recent unidirectional spectral refinement framework, but naturally integrates map consistency constraints into the refinement. Beyond that, we show further that our formulation can be adapted to recover the underlying isometry among near-isometric shape collections with a theoretical guarantee, which is absent in the other spectral map synchronization frameworks. We demonstrate that our method improves the accuracy compared to the competing methods when synchronizing correspondences in both near-isometric and heterogeneous shape collections, but also significantly outperforms the baselines in terms of map consistency.</p>
            <p class="pubothers">Proc. Symposium on Geometry Processing, 2020</p>
            <div class="pubclick">
                <a href="html/ConsistentZoomOut.html" class="pubbutton">[pdf 1.2M]</a>
                <a href="https://github.com/ruqihuang/SGP2020_ConsistentZoomOut" class="pubbutton">[code]</a>
                <a href="https://www.youtube.com/watch?v=WhduuY9o8QQ&t=24s" class="pubbutton">[video]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('ConsistentZoomOut')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/OperatorNet.png">
        </div>
        <div id="OperatorNet" class="pubtxt">
            <h3 class="pubtitle">OperatorNet: Recovering 3D Shapes From Difference Operators</h3>
            <p class="pubothers">Ruqi Huang, Marie-Julie Rakotosaona, Panos Achlioptas, Leonidas Guibas, Maks Ovsjanikov</p>
            <p class="pubabstract"><strong>Abstract:</strong>  This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications.</p>
            <p class="pubothers">Proc. International Conference on Computer Vision(ICCV), 2019</p>
            <div class="pubclick">
                <a href="html/OperatorNet.html" class="pubbutton">[pdf 13.1M]</a>
                <a href="https://github.com/mrakotosaon/operatornet" class="pubbutton">[code]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('OperatorNet')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/LimitShape.png">
        </div>
        <div id="LimitShape" class="pubtxt">
            <h3 class="pubtitle">Limit Shape – A Tool for Understanding Shape Differences and Variablity in 3D Model Collections</h3>
            <p class="pubothers">Ruqi Huang, Panos Achlioptas, Leonidas Guibas, Maks Ovsjanikov</p>
            <p class="pubabstract"><strong>Abstract:</strong>  We propose a novel construction for extracting a central or limit shape in a shape collection, connected via a functional map network. Our approach is based on enriching the latent space induced by a functional map network with an additional natural metric structure. We call this shape-like dual object the limit shape and show that its construction avoids many of the biases introduced by selecting a fixed base shape or template. We also show that shape differences between real shapes and the limit shape can be computed and characterize the unique properties of each shape in a collection – leading to a compact and rich shape representation. We demonstrate the utility of this representation in a range of shape analysis tasks, including improving functional maps in difficult situations through the mediation of limit shapes, understanding and visualizing the variability within and across different shape classes, and several others. In this way, our analysis sheds light on the missing geometric structure in previously used latent functional spaces, demonstrates how these can be addressed and finally enables a compact and meaningful shape representation useful in a variety of practical applications.</p>
            <p class="pubothers">Proc. Symposium on Geometry Processing, 2019</p>
            <div class="pubclick">
                <a href="html/LimitShape.html" class="pubbutton">[pdf 1.5M]</a>
                <a href="https://github.com/ruqihuang/LimitShape" class="pubbutton">[code]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('LimitShape')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/AMRSAM.png">
        </div>
        <div id="AMRSAM" class="pubtxt">
            <h3  class="pubtitle">Adjoint Map Representation for Shape Analysis and Matching</h3>
            <p class="pubothers">Ruqi Huang, Maks Ovsjanikov</p>
            <p class="pubabstract"><strong>Abstract:</strong>  In this paper, we propose to consider the adjoint operators of functional maps, and demonstrate their utility in several tasks in geometry processing. Unlike a functional map, which represents a correspondence simply using the pull-back of function values, the adjoint operator reflects both the map and its distortion with respect to given inner products. We argue that this property of adjoint operators and especially their relation to the map inverse under the choice of different inner products, can be useful in applications including bi-directional shape matching, shape exploration, and pointwise map recovery among others. In particular, in this paper, we show that the adjoint operators can be used within the cycle-consistency framework to encode and reveal the presence or lack of consistency between distortions in a collection, in a way that is complementary to the previously used purely map-based consistency measures. We also show how the adjoint can be used for matching pairs of shapes, by accounting for maps in both directions, can help in recovering point-to-point maps from their functional counterparts, and describe how it can shed light on the role of functional basis selection.</p>
            <p class="pubothers">Proc. Symposium on Geometry Processing, 2017</p>
            <div class="pubclick">
                <a href="html/AMRSAM.html" class="pubbutton">[pdf 18.6M]</a>
                <a href="https://github.com/ruqihuang/AdjointFmaps" class="pubbutton">[code]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('AMRSAM')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/OSFMSDO.png">
        </div>
        <div id="OSFMSDO" class="pubtxt">
            <h3 class="pubtitle">On the Stability of Functional Maps and Shape Difference Operators</h3>
            <p class="pubothers">Ruqi Huang, Frederic Chazal, Maks Ovsjanikov</p>
            <p class="pubabstract"><strong>Abstract:</strong>  In this paper, we provide stability guarantees for two frameworks that are based on the notion of functional maps – the shape difference operators and the framework which is used to analyze and visualize the deformations between shapes induced by a functional map. We consider two types of perturbations in our analysis: one is on the input shapes and the other is on the change in scale. In theory, we formulate and justify the robustness that has been observed in practical implementations of those frameworks. Inspired by our theoretical results, we propose a pipeline for constructing shape difference operators on point clouds and show numerically that the results are robust and informative. In particular, we show that both the shape difference operators and the derived areas of highest distortion are stable with respect to changes in shape representation and change of scale. Remarkably, this is in contrast with the well-known instability of the eigenfunctions of the Laplace-Beltrami operator computed on point clouds compared to those obtained on triangle meshes.</p>
            <p class="pubothers">Computer Graphics Forum, 2017</p>
            <div class="pubclick">
                <a href="html/OSFMSDO.html" class="pubbutton">[pdf 18.5M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('OSFMSDO')">[BibTex]</a>
            </div>
        </div>
    </div>
    <div class="pubdiv">
        <div class="pubimg">
            <img width="200px" src="img/GHAFSURTG.png">
        </div>
        <div id="GHAFSURTG" class="pubtxt">
            <h3 class="pubtitle">Gromov-Hausdorff Approximation of Filamentary Structures Using Reeb-Type Graphs</h3>
            <p class="pubothers">Frederic Chazal, Ruqi Huang, Jian Sun</p>
            <p class="pubabstract"><strong>Abstract:</strong>  In many real-world applications data appear to be sampled around 1-dimensional filamentary structures that can be seen as topological metric graphs. In this paper we address the metric reconstruction problem of such filamentary structures from data sampled around them. We prove that they can be approximated, with respect to the Gromov-Hausdorff distance by well-chosen Reeb graphs (and some of their variants) and provide an efficient and easy to implement algorithm to compute such approximations in almost linear time. We illustrate the performances of our algorithm on a few data sets.</p>
            <p class="pubothers">Discrete Computational Geometry, 2015</p>
            <div class="pubclick">
                <a href="html/GHAFSURTG.html" class="pubbutton">[pdf 3.0M]</a>
                <a href="javascript:void(0)" onclick="redirectToPageB('GHAFSURTG')">[BibTex]</a>
            </div>
        </div>
    </div>
</div>

<div id="Group" class="tabcontent">
    <h2>Group</h2>
    <div class="groupdiv">
        <div class="grouptxt">
            <h3>PhD Students</h3>
            <ul>
                <li>Jinzhi Zhang, 2022-now</li>
                <li>Yun Zhao, 2021-now</li>
                <li>Yanchen Guo, 2021-now</li>
                <li>Mingze Sun, 2022-now</li>
                <li>Yujia Chen, 2022-now</li>
                <li>Fengdi Zhang, 2022-now</li>
                <li>Xinyu Jiang, 2023-now</li>
                <li>Chen Guo, 2024-now</li>
                <li>Puhua Jiang, 2021-2025</li>
            </ul>
            <h3>MPhil Students</h3>
            <ul>
                <li>Zhangquan Chen, 2023-now</li>
                <li>Zhejia Cai, 2023-now</li>
                <li>Xiaoyu Hu, 2023-now</li>
                <li>Junhao Chen, 2024-now</li>
                <li>Jingjia Mao, 2024-now</li>
                <li>Cheng Zeng, 2024-now</li>
                <li>Keyi Chen, 2024-now</li>
                <li>Zijian Cheng, 2024-now</li>
                <li>Kaisan Li, 2020-2023</li>
                <li>Leyao Liu, 2020-2023</li>
                <li>Xuechao Chen, 2020-2023</li>
                <li>Zequn Chen, 2021-2024</li>
                <li>Chen Guo, 2021-2024</li>
                <li>Ting Zhang, 2021-2024</li>
                <li>Yurun Chen, 2022-2025</li>
                <li>Jin Wang, 2022-2025</li>
                <li>Shiwei Mao, 2022-2025</li>
                <li>Guochen Shao, 2022-2025</li>
                <li>Yunqi Zhao, 2022-2025</li>
            </ul>
        </div>
    </div>
</div>

</body>

<style>
    @import "css/index.css";
</style>

<script src="js/index.js"></script>

<style>
    a {
        color: royalblue;
        text-decoration: none;
    }

    a:visited {
        color: royalblue;
    }
</style>

<!--<script src="js/tabbar.js"></script>-->
<!--<script>-->
<!--    window.onload = function () {-->
<!--        iniStat()-->
<!--    }-->
<!--    function iniStat() {-->
<!--        changeCategory(event, 'Home');-->
<!--    }-->
<!--</script>-->

</html>
